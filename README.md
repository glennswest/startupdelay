# Startup Delay Testing Suite

This testing suite is designed to measure and monitor system responsiveness and startup delays on OpenShift cluster nodes, particularly master nodes. It works with both cgroup v1 and cgroup v2 environments.

## Purpose

The test suite measures system scheduling delays by running a continuous loop with 0.5-second sleep intervals and tracking the actual time differences between iterations. Delays beyond the expected 0.5 seconds indicate system responsiveness issues, which can help identify:

- Process scheduling delays
- System resource contention  
- Container runtime performance issues
- Node-level performance degradation

## Files Overview

| File | Purpose |
|------|---------|
| `runtest.sh` | Main test launcher - starts the test in background, writes to log file |
| `runpipedtest.sh` | Alternative launcher - real-time piped processing, no log file |
| `test.sh` | Core test script that measures timing intervals |
| `watchit.sh` | Monitoring wrapper using `watch` command (for log file method) |
| `watchpipedtest.sh` | Monitoring wrapper for piped test method |
| `watch.py` | Python script that analyzes results and tracks min/max values |
| `minmax.json` | Historical record of minimum and maximum delay changes |
| `disable_timers.sh` | CoreOS optimization script - disables systemd timers for cleaner long-running tests |

## Quick Start

### Method 1: File-Based Testing (Traditional)

```bash
# Start the startup delay test (writes to test.log)
./runtest.sh
```

This will:
- Run the test in the background
- Generate output in `test.log`
- Continue running until manually stopped

#### Monitoring File-Based Test

```bash
# Watch real-time analysis of results
./watchit.sh
```

This will display:
- Current minimum and maximum delay times
- Line count of processed entries  
- Alerts when new min/max records are detected

### Method 2: Piped Testing (Real-time)

```bash
# Start the piped test (real-time processing, no log file)
./runpipedtest.sh
```

This will:
- Run the test with real-time data processing
- Pipe output directly to analysis script
- Process data immediately without file I/O overhead
- Continue running until manually stopped

#### Monitoring Piped Test

```bash
# Watch real-time minmax.json updates
./watchpipedtest.sh
```

This will display:
- Live updates to the minmax.json file as records change
- Real-time tracking of new minimum/maximum values

### CoreOS Optimization (Long-running Tests)

For extended testing periods on CoreOS systems, disable systemd timers to reduce background interference:

```bash
# Disable systemd timers that can interfere with timing precision
sudo ./disable_timers.sh
```

This will:
- Stop and mask systemd-tmpfiles-clean.timer
- Stop and mask logrotate.timer  
- Stop and mask unbound-anchor.timer
- Reduce background system activity for more accurate measurements

**Note**: Only run this on test systems, not production environments.

### Stopping Either Test

```bash
# Find and stop the test process
pkill -f "test.sh"
# or
ps aux | grep test.sh
kill <PID>
```

## Detailed Component Description

### runtest.sh
Simple launcher script that:
- Starts `test.sh` in the background
- Redirects output to `test.log`
- Allows the test to run independently

### runpipedtest.sh
Alternative launcher for real-time processing that:
- Cleans up previous state files (`.maxmin`, `minmax.json`)
- Pipes `test.sh` output directly to `watch.py`
- Eliminates file I/O overhead for more responsive analysis
- Runs the entire pipeline in the background
- Provides immediate data processing without buffering delays

### test.sh
Core measurement script that:
- Records precise timestamps using `date +%s.%N` (nanosecond precision)
- Sleeps for 0.5 seconds each iteration
- Calculates actual time differences between iterations
- Outputs human-readable timestamps and timing data
- Runs in an infinite loop until terminated

**Expected behavior**: Time differences should be approximately 0.500-0.505 seconds under normal conditions.

### watchit.sh
Monitoring wrapper for file-based tests that:
- Uses the `watch` command to run `watch.py` repeatedly
- Provides real-time updates of the analysis
- Refreshes the display automatically
- Works with `test.log` generated by `runtest.sh`

### watchpipedtest.sh
Monitoring wrapper for piped tests that:
- Uses `tail -f` to follow `minmax.json` updates in real-time
- Shows immediate changes to min/max values as they occur
- Displays the JSON format output directly
- Provides continuous monitoring of the piped test results

### watch.py
Analysis and tracking script that:
- Parses `test.log` for timing measurements
- Maintains running minimum and maximum delay values
- Detects when new records are set
- Writes historical data to `minmax.json`
- Maintains current state in `.maxmin` file
- Provides console output when min/max values change

**Key features**:
- Initializes with min=100.0, max=-100.0 for first-run detection
- Tracks line numbers for easy log correlation
- Preserves full log lines where records were set
- Handles decimal parsing and validation

### disable_timers.sh
CoreOS system optimization script that:
- Stops and masks `systemd-tmpfiles-clean.timer` (temporary file cleanup)
- Stops and masks `logrotate.timer` (log rotation operations)
- Stops and masks `unbound-anchor.timer` (DNS anchor updates)
- Reduces background systemd activity that can interfere with timing precision
- Optimizes system for extended testing periods

**Important**: Only use on test systems, not production environments. These timers perform important maintenance tasks in normal operations.

## Output Format

### Console Output (from watch.py)
```
Min Value Line: Current time: 2025-08-25 13:29:53.173, Time difference from previous: 0.502227 seconds
Max Value Line: Current time: 2025-08-25 13:31:52.415, Time difference from previous: 0.516031 seconds  
Min = 0.502227 Max = 0.516031 Lines = 238
```

### minmax.json Format
```json
{
    "min": 0.502227,
    "max": 0.516031,
    "line": "Current time: 2025-08-25 13:31:52.415, Time difference from previous: 0.516031 seconds\n",
    "lineno": 238
}
```

Each entry records:
- `min`: Current minimum delay observed
- `max`: Current maximum delay observed  
- `line`: The exact log line where this min/max was recorded
- `lineno`: Line number in the log file

## Choosing the Right Test Method

### File-Based Testing (`runtest.sh`)
**Best for:**
- Long-term data collection and analysis
- Post-processing of historical data
- Environments where you need persistent logs
- Debugging and troubleshooting specific timing events
- Integration with log analysis tools

**Advantages:**
- Preserves complete historical data in `test.log`
- Can analyze data after test completion
- Easier to correlate with system events by timestamp
- More robust if monitoring process is interrupted

### Piped Testing (`runpipedtest.sh`)
**Best for:**
- Real-time monitoring and alerting
- Reduced disk I/O overhead
- Memory-constrained environments
- Live performance monitoring dashboards
- Environments where disk space is limited

**Advantages:**
- Lower system overhead (no file writes)
- Immediate data processing
- Reduced disk I/O that might affect timing measurements
- More responsive to rapid changes in system performance

### CoreOS Timer Optimization
**When to use `disable_timers.sh`:**
- Long-running tests (>24 hours)
- High-precision timing requirements
- Test environments (never production)
- When baseline measurements show interference from system maintenance

**Impact:**
- Reduces background system activity
- Improves timing measurement precision
- May show 0.001-0.005 second improvement in minimum delays
- Most effective on busy systems with frequent maintenance tasks

## Interpreting Results

### Normal Behavior
- **Expected range**: 0.500-0.510 seconds
- **Baseline minimum**: ~0.500-0.502 seconds
- **Acceptable maximum**: <0.520 seconds

### Warning Signs
- **High maximum delays**: >0.600 seconds indicate system stress
- **Increasing trend**: Growing maximum values over time
- **Wide variance**: Large gaps between min/max suggest inconsistent performance

### Critical Issues  
- **Extreme delays**: >1.0 second indicates severe system issues
- **Frequent spikes**: Regular high delays suggest resource contention
- **Baseline shift**: Minimum values increasing over time

## Platform Compatibility

### OpenShift Environments
- **Master nodes**: Preferred deployment location
- **Worker nodes**: Also supported but may show different patterns
- **cgroup v1**: Fully supported
- **cgroup v2**: Fully supported

### System Requirements
- Bash shell
- Python 3.x
- Standard Unix utilities (`date`, `sleep`, `watch`)
- Write permissions in the current directory

## Troubleshooting

### Common Issues

**Test not starting**:
```bash
# Check if scripts are executable
chmod +x runtest.sh runpipedtest.sh test.sh watchit.sh watchpipedtest.sh disable_timers.sh

# Verify dependencies
which python3
which watch
```

**No output in monitoring**:
```bash
# Check if test.log exists and is being written
ls -la test.log
tail -f test.log
```

**Python script errors**:
```bash
# Run analysis manually to see errors
python3 watch.py
```

**Piped test issues**:
```bash
# Check if piped process is running
ps aux | grep "test.sh.*python"

# Verify minmax.json is being updated
ls -la minmax.json
tail -f minmax.json

# If piped test stops responding, restart it
pkill -f "test.sh.*python"
./runpipedtest.sh
```

**Timer disabling issues** (CoreOS):
```bash
# Check timer status
systemctl status systemd-tmpfiles-clean.timer
systemctl status logrotate.timer
systemctl status unbound-anchor.timer

# Re-enable timers if needed (after testing)
systemctl unmask systemd-tmpfiles-clean.timer
systemctl unmask logrotate.timer
systemctl unmask unbound-anchor.timer
systemctl start systemd-tmpfiles-clean.timer
systemctl start logrotate.timer
systemctl start unbound-anchor.timer
```

### Log File Management

The test generates continuous output. For long-running tests, consider:

```bash
# Rotate logs periodically
mv test.log test.log.$(date +%Y%m%d_%H%M%S)
# Test will create a new test.log automatically
```

### Performance Impact

This test suite has minimal system impact:

**File-based method (`runtest.sh`)**:
- Single background process
- 0.5-second sleep intervals
- Lightweight Python analysis
- Small log file generation rate (~2 entries/second)
- Disk I/O: ~200 bytes/second

**Piped method (`runpipedtest.sh`)**:
- Single background process with piped data flow
- 0.5-second sleep intervals  
- Immediate Python processing
- No disk I/O for test data (only minmax.json updates)
- Slightly lower CPU overhead due to eliminated file writes

**With timer optimization (`disable_timers.sh`)**:
- Reduced system maintenance interruptions
- Lower background process activity
- Improved timing precision for extended tests
- No impact on test process resource usage

## Use Cases

### Performance Monitoring
- Continuous monitoring of node responsiveness
- Baseline establishment for cluster nodes
- Performance regression detection

### Troubleshooting
- Identifying intermittent scheduling issues
- Correlating delays with system events
- Validating performance optimizations

### Capacity Planning
- Understanding node performance characteristics
- Measuring impact of workload changes
- Establishing SLA baselines

## Data Retention

- `test.log`: Grows continuously, rotate as needed
- `minmax.json`: Append-only historical record
- `.maxmin`: Current state file, can be deleted to reset tracking

## Validation Testing

### Validation1 Test Run

The `validation1/` directory contains results from a comprehensive 13.5-hour test run that validates the effectiveness and reliability of the startup delay testing suite.

#### Test Environment
- **OpenShift Version**: 4.16.46
- **Kubernetes Version**: v1.29.14+a6b193c
- **Cluster Configuration**: 3 control-plane/master nodes + 3 worker nodes
- **Test Duration**: ~13.5 hours (August 25, 13:29 - August 26, 03:01)
- **Total Measurements**: 155,248 timing samples

#### Key Results
```
Baseline Performance:
- Minimum Delay: 0.502227 seconds
- Maximum Delay: 1.27347 seconds
- Average Expected: ~0.500 seconds (sleep interval)
```

#### Performance Analysis
The validation1 results demonstrate typical OpenShift master node behavior:

**Excellent Baseline**: The minimum delay of 0.502227 seconds shows the system can consistently achieve near-perfect scheduling precision.

**System Stress Events**: The maximum delay of 1.27347 seconds (occurring at line 96,489) indicates occasional system stress, likely due to:
- Kubernetes control plane operations
- Container scheduling activities  
- System maintenance tasks
- Resource garbage collection

**Long-term Stability**: Over 13.5 hours of continuous monitoring, the system maintained stable baseline performance with only occasional spikes.

#### Files in validation1/
- `test.log`: Complete timing measurements (13MB, 155,248 entries)
- `minmax.json`: Historical min/max tracking (2.6KB, 81 updates)
- `nodes.txt`: Cluster node information
- `version.txt`: OpenShift/Kubernetes version details

#### Validation Insights
1. **Normal Operation Range**: 99%+ of measurements fell within 0.500-0.520 seconds
2. **Stress Indicators**: Delays >0.600 seconds occurred occasionally, indicating normal system activity
3. **Critical Events**: The 1.27-second spike represents a significant but recoverable system event
4. **Monitoring Effectiveness**: The suite successfully detected and recorded all performance variations

This validation confirms the suite's ability to:
- Detect both normal performance variations and system stress events
- Maintain accurate long-term tracking
- Provide actionable performance insights for OpenShift operations

## Integration

This suite can be integrated with monitoring systems by:
- Parsing `minmax.json` for metrics collection
- Monitoring log growth rates
- Alerting on delay thresholds
- Correlating with other system metrics 